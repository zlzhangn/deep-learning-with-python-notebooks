{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHQ8RZ0o5vKU"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Third Edition](https://www.manning.com/books/deep-learning-with-python-third-edition). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "The book's contents are available online at [deeplearningwithpython.io](https://deeplearningwithpython.io)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-RQegMO5vKV"
      },
      "outputs": [],
      "source": [
        "!pip install keras keras-hub --upgrade -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_8wfDJH5vKV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Avrfph6w5vKV"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import os\n",
        "from IPython.core.magic import register_cell_magic\n",
        "\n",
        "@register_cell_magic\n",
        "def backend(line, cell):\n",
        "    current, required = os.environ.get(\"KERAS_BACKEND\", \"\"), line.split()[-1]\n",
        "    if current == required:\n",
        "        get_ipython().run_cell(cell)\n",
        "    else:\n",
        "        print(\n",
        "            f\"This cell requires the {required} backend. To run it, change KERAS_BACKEND to \"\n",
        "            f\"\\\"{required}\\\" at the top of the notebook, restart the runtime, and rerun the notebook.\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZziaz_E5vKV"
      },
      "source": [
        "## A deep dive on Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhfjLU_i5vKV"
      },
      "source": [
        "### A spectrum of workflows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKyjsKfm5vKV"
      },
      "source": [
        "### Different ways to build Keras models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z6YmH-r5vKV"
      },
      "source": [
        "#### The Sequential model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP3Y6VBL5vKV"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.Dense(10, activation=\"softmax\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR9cYrqY5vKV"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential()\n",
        "model.add(layers.Dense(64, activation=\"relu\"))\n",
        "model.add(layers.Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qo0fQtk5vKV"
      },
      "outputs": [],
      "source": [
        "model.weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtPMPgXw5vKV"
      },
      "outputs": [],
      "source": [
        "model.build(input_shape=(None, 3))\n",
        "model.weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnMk9V-V5vKV"
      },
      "outputs": [],
      "source": [
        "model.summary(line_length=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFe3jAFb5vKW"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential(name=\"my_example_model\")\n",
        "model.add(layers.Dense(64, activation=\"relu\", name=\"my_first_layer\"))\n",
        "model.add(layers.Dense(10, activation=\"softmax\", name=\"my_last_layer\"))\n",
        "model.build((None, 3))\n",
        "model.summary(line_length=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFKe-yaS5vKW"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.Input(shape=(3,)))\n",
        "model.add(layers.Dense(64, activation=\"relu\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SDrzCAs5vKW"
      },
      "outputs": [],
      "source": [
        "model.summary(line_length=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdMHSl-o5vKW"
      },
      "outputs": [],
      "source": [
        "model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "model.summary(line_length=80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQFEB2gc5vKW"
      },
      "source": [
        "#### The Functional API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KIjeLE-5vKW"
      },
      "source": [
        "##### A simple example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb1Lw_gf5vKW"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(3,), name=\"my_input\")\n",
        "features = layers.Dense(64, activation=\"relu\")(inputs)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name=\"my_functional_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O_NdTWP5vKW"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(3,), name=\"my_input\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoU_GEhH5vKW"
      },
      "outputs": [],
      "source": [
        "inputs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cuvRQJU5vKW"
      },
      "outputs": [],
      "source": [
        "inputs.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaK8YWKy5vKW"
      },
      "outputs": [],
      "source": [
        "features = layers.Dense(64, activation=\"relu\")(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeGzl14x5vKW"
      },
      "outputs": [],
      "source": [
        "features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpU7946z5vKW"
      },
      "outputs": [],
      "source": [
        "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name=\"my_functional_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxA6Xf4F5vKW"
      },
      "outputs": [],
      "source": [
        "model.summary(line_length=80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43-eiNaq5vKW"
      },
      "source": [
        "##### Multi-input, multi-output models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awYRXteZ5vKW"
      },
      "outputs": [],
      "source": [
        "vocabulary_size = 10000\n",
        "num_tags = 100\n",
        "num_departments = 4\n",
        "\n",
        "title = keras.Input(shape=(vocabulary_size,), name=\"title\")\n",
        "text_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\n",
        "tags = keras.Input(shape=(num_tags,), name=\"tags\")\n",
        "\n",
        "features = layers.Concatenate()([title, text_body, tags])\n",
        "features = layers.Dense(64, activation=\"relu\", name=\"dense_features\")(features)\n",
        "\n",
        "priority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features)\n",
        "department = layers.Dense(\n",
        "    num_departments, activation=\"softmax\", name=\"department\"\n",
        ")(features)\n",
        "\n",
        "model = keras.Model(\n",
        "    inputs=[title, text_body, tags],\n",
        "    outputs=[priority, department],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z40LZs6d5vKW"
      },
      "source": [
        "##### Training a multi-input, multi-output model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV55dv8b5vKW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_samples = 1280\n",
        "\n",
        "title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
        "text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
        "tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\n",
        "\n",
        "priority_data = np.random.random(size=(num_samples, 1))\n",
        "department_data = np.random.randint(0, num_departments, size=(num_samples, 1))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=[\"mean_squared_error\", \"sparse_categorical_crossentropy\"],\n",
        "    metrics=[[\"mean_absolute_error\"], [\"accuracy\"]],\n",
        ")\n",
        "model.fit(\n",
        "    [title_data, text_body_data, tags_data],\n",
        "    [priority_data, department_data],\n",
        "    epochs=1,\n",
        ")\n",
        "model.evaluate(\n",
        "    [title_data, text_body_data, tags_data], [priority_data, department_data]\n",
        ")\n",
        "priority_preds, department_preds = model.predict(\n",
        "    [title_data, text_body_data, tags_data]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzlGjDWX5vKW"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss={\n",
        "        \"priority\": \"mean_squared_error\",\n",
        "        \"department\": \"sparse_categorical_crossentropy\",\n",
        "    },\n",
        "    metrics={\n",
        "        \"priority\": [\"mean_absolute_error\"],\n",
        "        \"department\": [\"accuracy\"],\n",
        "    },\n",
        ")\n",
        "model.fit(\n",
        "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
        "    {\"priority\": priority_data, \"department\": department_data},\n",
        "    epochs=1,\n",
        ")\n",
        "model.evaluate(\n",
        "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
        "    {\"priority\": priority_data, \"department\": department_data},\n",
        ")\n",
        "priority_preds, department_preds = model.predict(\n",
        "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o0WdgFn5vKX"
      },
      "source": [
        "##### The power of the Functional API: Access to layer connectivity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_nIJnG05vKX"
      },
      "source": [
        "###### Plotting layer connectivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAd8oPHA5vKX"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, \"ticket_classifier.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdSX8Ah35vKX"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(\n",
        "    model,\n",
        "    \"ticket_classifier_with_shape_info.png\",\n",
        "    show_shapes=True,\n",
        "    show_layer_names=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oLg-VL15vKX"
      },
      "source": [
        "###### Feature extraction with a Functional model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNBPB6Vj5vKX"
      },
      "outputs": [],
      "source": [
        "model.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hsgoyhrr5vKX"
      },
      "outputs": [],
      "source": [
        "model.layers[3].input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDvT5QgY5vKX"
      },
      "outputs": [],
      "source": [
        "model.layers[3].output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr2tPwS85vKX"
      },
      "outputs": [],
      "source": [
        "features = model.layers[4].output\n",
        "difficulty = layers.Dense(3, activation=\"softmax\", name=\"difficulty\")(features)\n",
        "\n",
        "new_model = keras.Model(\n",
        "    inputs=[title, text_body, tags], outputs=[priority, department, difficulty]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVzZmdVw5vKX"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(\n",
        "    new_model,\n",
        "    \"updated_ticket_classifier.png\",\n",
        "    show_shapes=True,\n",
        "    show_layer_names=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sajAV1695vKX"
      },
      "source": [
        "#### Subclassing the Model class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFh6aFhi5vKX"
      },
      "source": [
        "##### Rewriting our previous example as a subclassed model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW-lKe7z5vKX"
      },
      "outputs": [],
      "source": [
        "class CustomerTicketModel(keras.Model):\n",
        "    def __init__(self, num_departments):\n",
        "        super().__init__()\n",
        "        self.concat_layer = layers.Concatenate()\n",
        "        self.mixing_layer = layers.Dense(64, activation=\"relu\")\n",
        "        self.priority_scorer = layers.Dense(1, activation=\"sigmoid\")\n",
        "        self.department_classifier = layers.Dense(\n",
        "            num_departments, activation=\"softmax\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        title = inputs[\"title\"]\n",
        "        text_body = inputs[\"text_body\"]\n",
        "        tags = inputs[\"tags\"]\n",
        "\n",
        "        features = self.concat_layer([title, text_body, tags])\n",
        "        features = self.mixing_layer(features)\n",
        "        priority = self.priority_scorer(features)\n",
        "        department = self.department_classifier(features)\n",
        "        return priority, department"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pwP3Rsb5vKX"
      },
      "outputs": [],
      "source": [
        "model = CustomerTicketModel(num_departments=4)\n",
        "\n",
        "priority, department = model(\n",
        "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smZfKLiS5vKX"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=[\"mean_squared_error\", \"sparse_categorical_crossentropy\"],\n",
        "    metrics=[[\"mean_absolute_error\"], [\"accuracy\"]],\n",
        ")\n",
        "model.fit(\n",
        "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
        "    [priority_data, department_data],\n",
        "    epochs=1,\n",
        ")\n",
        "model.evaluate(\n",
        "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
        "    [priority_data, department_data],\n",
        ")\n",
        "priority_preds, department_preds = model.predict(\n",
        "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjJpg3i45vKX"
      },
      "source": [
        "##### Beware: What subclassed models don't support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0VEomEk5vKX"
      },
      "source": [
        "#### Mixing and matching different components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlEEA8tZ5vKX"
      },
      "outputs": [],
      "source": [
        "class Classifier(keras.Model):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        if num_classes == 2:\n",
        "            num_units = 1\n",
        "            activation = \"sigmoid\"\n",
        "        else:\n",
        "            num_units = num_classes\n",
        "            activation = \"softmax\"\n",
        "        self.dense = layers.Dense(num_units, activation=activation)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.dense(inputs)\n",
        "\n",
        "inputs = keras.Input(shape=(3,))\n",
        "features = layers.Dense(64, activation=\"relu\")(inputs)\n",
        "outputs = Classifier(num_classes=10)(features)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZAZL8435vKX"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(64,))\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(inputs)\n",
        "binary_classifier = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "class MyModel(keras.Model):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.dense = layers.Dense(64, activation=\"relu\")\n",
        "        self.classifier = binary_classifier\n",
        "\n",
        "    def call(self, inputs):\n",
        "        features = self.dense(inputs)\n",
        "        return self.classifier(features)\n",
        "\n",
        "model = MyModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFVfV1KJ5vKX"
      },
      "source": [
        "#### Remember: Use the right tool for the job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhRCHRSc5vKY"
      },
      "source": [
        "### Using built-in training and evaluation loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yH1kGqI95vKY"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "def get_mnist_model():\n",
        "    inputs = keras.Input(shape=(28 * 28,))\n",
        "    features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "    features = layers.Dropout(0.5)(features)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
        "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
        "train_images, val_images = images[10000:], images[:10000]\n",
        "train_labels, val_labels = labels[10000:], labels[:10000]\n",
        "\n",
        "model = get_mnist_model()\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    epochs=3,\n",
        "    validation_data=(val_images, val_labels),\n",
        ")\n",
        "test_metrics = model.evaluate(test_images, test_labels)\n",
        "predictions = model.predict(test_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe7sYFBh5vKY"
      },
      "source": [
        "#### Writing your own metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "zvP4IASHwqqf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vkdxYhK5vKY"
      },
      "outputs": [],
      "source": [
        "from keras import ops\n",
        "\n",
        "class RootMeanSquaredError(keras.metrics.Metric):\n",
        "    def __init__(self, name=\"rmse\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.mse_sum = self.add_weight(name=\"mse_sum\", initializer=\"zeros\")\n",
        "        self.total_samples = self.add_weight(\n",
        "            name=\"total_samples\", initializer=\"zeros\"\n",
        "        )\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = ops.one_hot(y_true, num_classes=ops.shape(y_pred)[1])\n",
        "        mse = ops.sum(ops.square(y_true - y_pred))\n",
        "        self.mse_sum.assign_add(mse)\n",
        "        num_samples = ops.shape(y_pred)[0]\n",
        "        self.total_samples.assign_add(num_samples)\n",
        "\n",
        "    def result(self):\n",
        "        return ops.sqrt(self.mse_sum / self.total_samples)\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.mse_sum.assign(0.)\n",
        "        self.total_samples.assign(0.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGf16k9t5vKY"
      },
      "outputs": [],
      "source": [
        "model = get_mnist_model()\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\", RootMeanSquaredError()],\n",
        ")\n",
        "model.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    epochs=3,\n",
        "    validation_data=(val_images, val_labels),\n",
        ")\n",
        "test_metrics = model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 代码解释"
      ],
      "metadata": {
        "id": "TuFLAgzrxSVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这段代码定义了一个自定义的 **Keras 指标（Metric）类** —— `RootMeanSquaredError`（RMSE，均方根误差），它继承自 `keras.metrics.Metric`。\n",
        "我们逐行来解释：\n",
        "\n",
        "---\n",
        "\n",
        "##### 🌱 1️⃣ 导入与类定义\n",
        "\n",
        "```python\n",
        "from keras import ops\n",
        "\n",
        "class RootMeanSquaredError(keras.metrics.Metric):\n",
        "```\n",
        "\n",
        "* `keras.metrics.Metric` 是所有自定义指标的基类。\n",
        "* `ops` 是 Keras 的后端操作模块（兼容 TensorFlow、JAX、Torch），提供了如 `ops.sum()`、`ops.sqrt()` 等张量运算。\n",
        "\n",
        "---\n",
        "\n",
        "##### 🌿 2️⃣ 初始化方法 `__init__`\n",
        "\n",
        "```python\n",
        "def __init__(self, name=\"rmse\", **kwargs):\n",
        "    super().__init__(name=name, **kwargs)\n",
        "    self.mse_sum = self.add_weight(name=\"mse_sum\", initializer=\"zeros\")\n",
        "    self.total_samples = self.add_weight(name=\"total_samples\", initializer=\"zeros\")\n",
        "```\n",
        "\n",
        "* `name=\"rmse\"` 给指标命名，训练时可以用 `metrics=[\"rmse\"]` 调用。\n",
        "* `self.add_weight()` 定义两个**状态变量（state variables）**，在每个 batch 更新：\n",
        "\n",
        "  * `mse_sum`：累计所有样本的平方误差和\n",
        "  * `total_samples`：累计样本数\n",
        "* 初始化为 0。\n",
        "\n",
        "---\n",
        "\n",
        "##### 🍃 3️⃣ 更新状态 `update_state`\n",
        "\n",
        "```python\n",
        "def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    y_true = ops.one_hot(y_true, num_classes=ops.shape(y_pred)[1])\n",
        "    mse = ops.sum(ops.square(y_true - y_pred))\n",
        "    self.mse_sum.assign_add(mse)\n",
        "    num_samples = ops.shape(y_pred)[0]\n",
        "    self.total_samples.assign_add(num_samples)\n",
        "```\n",
        "\n",
        "每处理一个 batch，Keras 会调用一次 `update_state()` 来更新指标值。\n",
        "\n",
        "* `y_true = ops.one_hot(...)`\n",
        "  将标签转为 one-hot 向量（因为预测输出是 softmax）。\n",
        "  比如 y_true = `[1, 2]` → one-hot → `[[0,1,0], [0,0,1]]`\n",
        "\n",
        "* `ops.square(y_true - y_pred)`：计算逐元素平方误差。\n",
        "\n",
        "* `ops.sum(...)`：对整个 batch 求和，得到该 batch 的总平方误差。\n",
        "\n",
        "* `assign_add()`：累加到全局的 `mse_sum` 和 `total_samples`。\n",
        "\n",
        "---\n",
        "\n",
        "##### 🌸 4️⃣ 计算结果 `result`\n",
        "\n",
        "```python\n",
        "def result(self):\n",
        "    return ops.sqrt(self.mse_sum / self.total_samples)\n",
        "```\n",
        "\n",
        "* 计算整体 RMSE = √(MSE)，即均方误差的平方根。\n",
        "\n",
        "---\n",
        "\n",
        "##### 🍂 5️⃣ 重置状态 `reset_state`\n",
        "\n",
        "```python\n",
        "def reset_state(self):\n",
        "    self.mse_sum.assign(0.)\n",
        "    self.total_samples.assign(0.)\n",
        "```\n",
        "\n",
        "* 每个 epoch 结束后，Keras 会调用此方法清零状态，避免跨 epoch 混淆。\n",
        "\n",
        "---\n",
        "\n",
        "##### ✅ 6️⃣ 总结流程图\n",
        "\n",
        "```\n",
        "初始化：\n",
        "  mse_sum = 0, total_samples = 0\n",
        "\n",
        "每批次：\n",
        "  计算当前 batch 的平方误差 → 累加到 mse_sum\n",
        "  计算样本数 → 累加到 total_samples\n",
        "\n",
        "最终结果：\n",
        "  rmse = sqrt(mse_sum / total_samples)\n",
        "\n",
        "epoch 结束：\n",
        "  reset_state()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "##### 📘 使用示例\n",
        "\n",
        "```python\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[RootMeanSquaredError()]\n",
        ")\n",
        "```\n",
        "\n",
        "训练时，Keras 会自动：\n",
        "\n",
        "* 在每个 batch 调用 `update_state`\n",
        "* 在每个 epoch 结束时计算 `result`\n",
        "* 在新 epoch 开始时调用 `reset_state`\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GGSmgILPxIP4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA8Fx3595vKY"
      },
      "source": [
        "#### Using callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIElkjYB5vKY"
      },
      "source": [
        "##### The EarlyStopping and ModelCheckpoint callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxDR3r7p5vKY"
      },
      "outputs": [],
      "source": [
        "callbacks_list = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor=\"accuracy\",\n",
        "        patience=1,\n",
        "    ),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"checkpoint_path.keras\",\n",
        "        monitor=\"val_loss\",\n",
        "        save_best_only=True,\n",
        "    ),\n",
        "]\n",
        "model = get_mnist_model()\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks_list,\n",
        "    validation_data=(val_images, val_labels),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC4OLu6E5vKY"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model(\"checkpoint_path.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOUpuDNp5vKY"
      },
      "source": [
        "#### Writing your own callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwKyhoQS5vKY"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs):\n",
        "        self.per_batch_losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        self.per_batch_losses.append(logs.get(\"loss\"))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        plt.clf()\n",
        "        plt.plot(\n",
        "            range(len(self.per_batch_losses)),\n",
        "            self.per_batch_losses,\n",
        "            label=\"Training loss for each batch\",\n",
        "        )\n",
        "        plt.xlabel(f\"Batch (epoch {epoch})\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.savefig(f\"plot_at_epoch_{epoch}\", dpi=300)\n",
        "        self.per_batch_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aYDRWyS5vKY"
      },
      "outputs": [],
      "source": [
        "model = get_mnist_model()\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    epochs=10,\n",
        "    callbacks=[LossHistory()],\n",
        "    validation_data=(val_images, val_labels),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPg9SI8t5vKY"
      },
      "source": [
        "#### Monitoring and visualization with TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33T7PCl45vKY"
      },
      "outputs": [],
      "source": [
        "model = get_mnist_model()\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "tensorboard = keras.callbacks.TensorBoard(\n",
        "    log_dir=\"/full_path_to_your_log_dir\",\n",
        ")\n",
        "model.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    epochs=10,\n",
        "    validation_data=(val_images, val_labels),\n",
        "    callbacks=[tensorboard],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhBMSppE5vKY"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /full_path_to_your_log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4G7XjkH5vKY"
      },
      "source": [
        "### Writing your own training and evaluation loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVrqzNlT5vKY"
      },
      "source": [
        "#### Training vs. inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJT86C-B5vKY"
      },
      "source": [
        "#### Writing custom training step functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn6eZgLs5vKY"
      },
      "source": [
        "##### A TensorFlow training step function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEauTdH-5vKZ"
      },
      "outputs": [],
      "source": [
        "%%backend tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "model = get_mnist_model()\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.Adam()\n",
        "\n",
        "def train_step(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(targets, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply(gradients, model.trainable_weights)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z6AJemq5vKZ"
      },
      "outputs": [],
      "source": [
        "%%backend tensorflow\n",
        "batch_size = 32\n",
        "inputs = train_images[:batch_size]\n",
        "targets = train_labels[:batch_size]\n",
        "loss = train_step(inputs, targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 代码解释"
      ],
      "metadata": {
        "id": "KGbLTE-TykZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这段代码展示了 **Keras + TensorFlow 自定义训练循环（custom training loop）** 的基本原理。\n",
        "我们逐行来看它做了什么：\n",
        "\n",
        "---\n",
        "\n",
        "####### 🌱 1️⃣ 环境与准备\n",
        "\n",
        "```python\n",
        "%%backend tensorflow\n",
        "import tensorflow as tf\n",
        "```\n",
        "\n",
        "* `%%backend tensorflow` 是 Jupyter 魔法命令，指定后端使用 TensorFlow。\n",
        "* `import tensorflow as tf` 导入 TensorFlow 库。\n",
        "\n",
        "---\n",
        "\n",
        "####### 🌿 2️⃣ 创建模型、损失函数与优化器\n",
        "\n",
        "```python\n",
        "model = get_mnist_model()\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.Adam()\n",
        "```\n",
        "\n",
        "* `get_mnist_model()` 是一个自定义函数（假设定义在前面），返回一个 MNIST 分类模型（比如卷积网络或全连接网络）。\n",
        "* `loss_fn` 是稀疏分类交叉熵：\n",
        "\n",
        "  * 适用于整数标签的多分类问题；\n",
        "  * 输入标签如 `[3, 1, 7, ...]`；\n",
        "  * 输出预测为 softmax 概率分布。\n",
        "* `optimizer = Adam()` 用来根据梯度更新参数。\n",
        "\n",
        "---\n",
        "\n",
        "####### 🍃 3️⃣ 定义训练步骤函数\n",
        "\n",
        "```python\n",
        "def train_step(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(targets, predictions)\n",
        "```\n",
        "\n",
        "######## ✳️ `tf.GradientTape()` 的作用\n",
        "\n",
        "* 这是 TensorFlow 的自动微分机制；\n",
        "* 它**记录**模型前向传播中涉及的所有可训练变量；\n",
        "* 之后可以调用 `tape.gradient()` 自动计算这些变量的梯度。\n",
        "\n",
        "流程：\n",
        "\n",
        "1. **前向传播（forward pass）**\n",
        "   `predictions = model(inputs, training=True)`\n",
        "   模型在训练模式下执行前向计算；\n",
        "2. **计算损失（loss）**\n",
        "   `loss_fn(targets, predictions)` 计算预测与真实标签之间的误差。\n",
        "\n",
        "---\n",
        "\n",
        "####### 🍀 4️⃣ 计算梯度并更新权重\n",
        "\n",
        "```python\n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply(gradients, model.trainable_weights)\n",
        "```\n",
        "\n",
        "* `tape.gradient(loss, model.trainable_weights)`\n",
        "  自动计算损失相对于每个可训练参数的梯度。\n",
        "* `optimizer.apply(gradients, model.trainable_weights)`\n",
        "  将梯度应用于参数（即执行一次反向传播 + 参数更新）。\n",
        "\n",
        "---\n",
        "\n",
        "####### 🌸 5️⃣ 返回损失\n",
        "\n",
        "```python\n",
        "    return loss\n",
        "```\n",
        "\n",
        "每次调用 `train_step()`，模型会完成：\n",
        "\n",
        "1. 前向传播\n",
        "2. 计算损失\n",
        "3. 自动求梯度\n",
        "4. 执行梯度下降（参数更新）\n",
        "5. 返回当前 batch 的损失值\n",
        "\n",
        "---\n",
        "\n",
        "####### 🔁 6️⃣ 在训练循环中使用\n",
        "\n",
        "你可以像这样调用它：\n",
        "\n",
        "```python\n",
        "for epoch in range(num_epochs):\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss_value = train_step(x_batch, y_batch)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss_value.numpy():.4f}\")\n",
        "```\n",
        "\n",
        "这样你就实现了一个完全**自定义的训练循环**。\n",
        "相比 `model.fit()`，这种方式能：\n",
        "\n",
        "* 完全控制训练逻辑；\n",
        "* 在训练过程中加入特殊操作（如梯度裁剪、对抗训练、可视化等）；\n",
        "* 便于研究实验性网络结构。\n",
        "\n",
        "---\n",
        "\n",
        "######## ✅ 总结逻辑流程图：\n",
        "\n",
        "```\n",
        "train_step(inputs, targets):\n",
        "    1. 开始记录计算图 (tf.GradientTape)\n",
        "    2. 前向传播: predictions = model(inputs)\n",
        "    3. 计算损失: loss = loss_fn(targets, predictions)\n",
        "    4. 自动微分: gradients = tape.gradient(loss, model.weights)\n",
        "    5. 应用梯度: optimizer.apply(gradients, model.weights)\n",
        "    6. 返回当前 loss\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "VmAmUFb7zvF4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M1-EPVQ5vKZ"
      },
      "source": [
        "##### A PyTorch training step function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQxldOTt5vKZ"
      },
      "outputs": [],
      "source": [
        "%%backend torch\n",
        "import torch\n",
        "\n",
        "model = get_mnist_model()\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.Adam()\n",
        "\n",
        "def train_step(inputs, targets):\n",
        "    predictions = model(inputs, training=True)\n",
        "    loss = loss_fn(targets, predictions)\n",
        "    loss.backward()\n",
        "    gradients = [weight.value.grad for weight in model.trainable_weights]\n",
        "    with torch.no_grad():\n",
        "        optimizer.apply(gradients, model.trainable_weights)\n",
        "    model.zero_grad()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkAYsW745vKZ"
      },
      "outputs": [],
      "source": [
        "%%backend torch\n",
        "batch_size = 32\n",
        "inputs = train_images[:batch_size]\n",
        "targets = train_labels[:batch_size]\n",
        "loss = train_step(inputs, targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lx1ZEDm5vKZ"
      },
      "source": [
        "##### A JAX training step function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs90gob85vKZ"
      },
      "outputs": [],
      "source": [
        "%%backend jax\n",
        "model = get_mnist_model()\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "def compute_loss_and_updates(\n",
        "    trainable_variables, non_trainable_variables, inputs, targets\n",
        "):\n",
        "    outputs, non_trainable_variables = model.stateless_call(\n",
        "        trainable_variables, non_trainable_variables, inputs, training=True\n",
        "    )\n",
        "    loss = loss_fn(targets, outputs)\n",
        "    return loss, non_trainable_variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD6tmvM45vKZ"
      },
      "outputs": [],
      "source": [
        "%%backend jax\n",
        "import jax\n",
        "\n",
        "grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPqAKU3c5vKZ"
      },
      "outputs": [],
      "source": [
        "%%backend jax\n",
        "optimizer = keras.optimizers.Adam()\n",
        "optimizer.build(model.trainable_variables)\n",
        "\n",
        "def train_step(state, inputs, targets):\n",
        "    (trainable_variables, non_trainable_variables, optimizer_variables) = state\n",
        "    (loss, non_trainable_variables), grads = grad_fn(\n",
        "        trainable_variables, non_trainable_variables, inputs, targets\n",
        "    )\n",
        "    trainable_variables, optimizer_variables = optimizer.stateless_apply(\n",
        "        optimizer_variables, grads, trainable_variables\n",
        "    )\n",
        "    return loss, (\n",
        "        trainable_variables,\n",
        "        non_trainable_variables,\n",
        "        optimizer_variables,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGRdmwfz5vKZ"
      },
      "outputs": [],
      "source": [
        "%%backend jax\n",
        "batch_size = 32\n",
        "inputs = train_images[:batch_size]\n",
        "targets = train_labels[:batch_size]\n",
        "\n",
        "trainable_variables = [v.value for v in model.trainable_variables]\n",
        "non_trainable_variables = [v.value for v in model.non_trainable_variables]\n",
        "optimizer_variables = [v.value for v in optimizer.variables]\n",
        "\n",
        "state = (trainable_variables, non_trainable_variables, optimizer_variables)\n",
        "loss, state = train_step(state, inputs, targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnddI84q5vKZ"
      },
      "source": [
        "#### Low-level usage of metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzVVCvD95vKZ"
      },
      "outputs": [],
      "source": [
        "from keras import ops\n",
        "\n",
        "metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "targets = ops.array([0, 1, 2])\n",
        "predictions = ops.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
        "metric.update_state(targets, predictions)\n",
        "current_result = metric.result()\n",
        "print(f\"result: {current_result:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYlBQUE75vKZ"
      },
      "outputs": [],
      "source": [
        "values = ops.array([0, 1, 2, 3, 4])\n",
        "mean_tracker = keras.metrics.Mean()\n",
        "for value in values:\n",
        "    mean_tracker.update_state(value)\n",
        "print(f\"Mean of values: {mean_tracker.result():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5R2ljqj5vKZ"
      },
      "outputs": [],
      "source": [
        "metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "targets = ops.array([0, 1, 2])\n",
        "predictions = ops.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
        "\n",
        "metric_variables = metric.variables\n",
        "metric_variables = metric.stateless_update_state(\n",
        "    metric_variables, targets, predictions\n",
        ")\n",
        "current_result = metric.stateless_result(metric_variables)\n",
        "print(f\"result: {current_result:.2f}\")\n",
        "\n",
        "metric_variables = metric.stateless_reset_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 代码解释"
      ],
      "metadata": {
        "id": "M7KX4xRZ1XWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "非常好 👍，这段代码演示了 **Keras 指标（Metric）对象的工作原理** ——\n",
        "以 `SparseCategoricalAccuracy`（稀疏分类准确率）为例，展示了如何手动计算模型预测的准确率。\n",
        "我们一步步来看 👇\n",
        "\n",
        "---\n",
        "\n",
        "###### 🧩 1️⃣ 导入 ops 并创建指标对象\n",
        "\n",
        "```python\n",
        "from keras import ops\n",
        "metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "```\n",
        "\n",
        "* `ops` 是 Keras 后端的通用操作接口（在不同后端如 TensorFlow、JAX、PyTorch 中保持一致的 API）。\n",
        "* `keras.metrics.SparseCategoricalAccuracy()` 表示稀疏分类准确率：\n",
        "\n",
        "  * 用于多分类任务；\n",
        "  * 真实标签 (`y_true`) 是整数索引，如 `[0, 1, 2]`；\n",
        "  * 预测结果 (`y_pred`) 是 softmax 概率向量，如 `[[0.8, 0.1, 0.1], ...]`。\n",
        "\n",
        "---\n",
        "\n",
        "###### 📊 2️⃣ 构造测试数据\n",
        "\n",
        "```python\n",
        "targets = ops.array([0, 1, 2])\n",
        "predictions = ops.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
        "```\n",
        "\n",
        "* `targets`：真实类别为\n",
        "\n",
        "  ```\n",
        "  样本1 → 类别0  \n",
        "  样本2 → 类别1  \n",
        "  样本3 → 类别2\n",
        "  ```\n",
        "* `predictions`：每个样本的预测概率向量\n",
        "\n",
        "  ```\n",
        "  样本1预测为类0（概率最高）  \n",
        "  样本2预测为类1  \n",
        "  样本3预测为类2\n",
        "  ```\n",
        "\n",
        "  因此，预测是完全正确的。\n",
        "\n",
        "---\n",
        "\n",
        "###### ⚙️ 3️⃣ 更新指标状态\n",
        "\n",
        "```python\n",
        "metric.update_state(targets, predictions)\n",
        "```\n",
        "\n",
        "####### 这一步的作用：\n",
        "\n",
        "* `update_state()` 会比较真实标签与预测结果；\n",
        "* 内部计算：\n",
        "\n",
        "  1. `pred_class = argmax(predictions, axis=-1)` → `[0, 1, 2]`\n",
        "  2. 与 `targets` `[0, 1, 2]` 比较；\n",
        "  3. 匹配成功的样本数 = 3；\n",
        "  4. 总样本数 = 3；\n",
        "  5. 暂存到内部状态（`metric.total`, `metric.count`）。\n",
        "\n",
        "---\n",
        "\n",
        "###### 📈 4️⃣ 获取计算结果\n",
        "\n",
        "```python\n",
        "current_result = metric.result()\n",
        "```\n",
        "\n",
        "* `result()` 根据内部状态计算准确率：\n",
        "  [\n",
        "  \\text{accuracy} = \\frac{\\text{正确样本数}}{\\text{总样本数}} = \\frac{3}{3} = 1.0\n",
        "  ]\n",
        "\n",
        "---\n",
        "\n",
        "###### 🖨️ 5️⃣ 打印结果\n",
        "\n",
        "```python\n",
        "print(f\"result: {current_result:.2f}\")\n",
        "```\n",
        "\n",
        "输出：\n",
        "\n",
        "```\n",
        "result: 1.00\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###### ✅ 总结流程\n",
        "\n",
        "| 步骤 | 函数                                                   | 含义                   |\n",
        "| -- | ---------------------------------------------------- | -------------------- |\n",
        "| 1  | `metric = keras.metrics.SparseCategoricalAccuracy()` | 创建指标对象               |\n",
        "| 2  | `metric.update_state(y_true, y_pred)`                | 更新内部状态（统计正确预测数与总样本数） |\n",
        "| 3  | `metric.result()`                                    | 计算当前准确率              |\n",
        "| 4  | `metric.reset_state()`（未展示）                          | 清空累计统计，准备下一轮计算       |\n",
        "\n",
        "---\n",
        "\n",
        "####### 💡拓展：如果预测不完全正确\n",
        "\n",
        "假如：\n",
        "\n",
        "```python\n",
        "predictions = ops.array([[1, 0, 0], [0, 0, 1], [0, 1, 0]])\n",
        "```\n",
        "\n",
        "即只有第一个样本预测正确，\n",
        "则：\n",
        "[\n",
        "\\text{accuracy} = \\frac{1}{3} \\approx 0.33\n",
        "]\n",
        "输出会变为：\n",
        "\n",
        "```\n",
        "result: 0.33\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vg0TsK_R1gOI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkPbZouI5vKZ"
      },
      "source": [
        "#### Using fit() with a custom training loop\n",
        "结合自定义训练循环，和keras内置循环的功能"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grYVptDr5vKZ"
      },
      "source": [
        "##### Customizing fit() with TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_2XmPff5vKZ"
      },
      "outputs": [],
      "source": [
        "%%backend tensorflow\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        inputs, targets = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = loss_fn(targets, predictions)\n",
        "        gradients = tape.gradient(loss, self.trainable_weights)\n",
        "        self.optimizer.apply(gradients, self.trainable_weights)\n",
        "\n",
        "        loss_tracker.update_state(loss)\n",
        "        return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [loss_tracker]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A6KEgF95vKZ"
      },
      "outputs": [],
      "source": [
        "%%backend tensorflow\n",
        "def get_custom_model():\n",
        "    inputs = keras.Input(shape=(28 * 28,))\n",
        "    features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "    features = layers.Dropout(0.5)(features)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "    model = CustomModel(inputs, outputs)\n",
        "    model.compile(optimizer=keras.optimizers.Adam())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djqk6SR05vKZ"
      },
      "outputs": [],
      "source": [
        "%%backend tensorflow\n",
        "model = get_custom_model()\n",
        "model.fit(train_images, train_labels, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDMjzAOA5vKa"
      },
      "source": [
        "##### Customizing fit() with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSNyI_5Z5vKa"
      },
      "outputs": [],
      "source": [
        "%%backend torch\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        inputs, targets = data\n",
        "        predictions = self(inputs, training=True)\n",
        "        loss = loss_fn(targets, predictions)\n",
        "\n",
        "        loss.backward()\n",
        "        trainable_weights = [v for v in self.trainable_weights]\n",
        "        gradients = [v.value.grad for v in trainable_weights]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.optimizer.apply(gradients, trainable_weights)\n",
        "\n",
        "        loss_tracker.update_state(loss)\n",
        "        return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [loss_tracker]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI9d8rOb5vKa"
      },
      "outputs": [],
      "source": [
        "%%backend torch\n",
        "def get_custom_model():\n",
        "    inputs = keras.Input(shape=(28 * 28,))\n",
        "    features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "    features = layers.Dropout(0.5)(features)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "    model = CustomModel(inputs, outputs)\n",
        "    model.compile(optimizer=keras.optimizers.Adam())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPd5lPbi5vKa"
      },
      "outputs": [],
      "source": [
        "%%backend torch\n",
        "model = get_custom_model()\n",
        "model.fit(train_images, train_labels, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmu2qDQ85vKa"
      },
      "source": [
        "##### Customizing fit() with JAX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DpZujlm5vKa"
      },
      "outputs": [],
      "source": [
        "%%backend jax\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "    def compute_loss_and_updates(\n",
        "        self,\n",
        "        trainable_variables,\n",
        "        non_trainable_variables,\n",
        "        inputs,\n",
        "        targets,\n",
        "        training=False,\n",
        "    ):\n",
        "        predictions, non_trainable_variables = self.stateless_call(\n",
        "            trainable_variables,\n",
        "            non_trainable_variables,\n",
        "            inputs,\n",
        "            training=training,\n",
        "        )\n",
        "        loss = loss_fn(targets, predictions)\n",
        "        return loss, non_trainable_variables\n",
        "\n",
        "    def train_step(self, state, data):\n",
        "        (\n",
        "            trainable_variables,\n",
        "            non_trainable_variables,\n",
        "            optimizer_variables,\n",
        "            metrics_variables,\n",
        "        ) = state\n",
        "        inputs, targets = data\n",
        "\n",
        "        grad_fn = jax.value_and_grad(\n",
        "            self.compute_loss_and_updates, has_aux=True\n",
        "        )\n",
        "\n",
        "        (loss, non_trainable_variables), grads = grad_fn(\n",
        "            trainable_variables,\n",
        "            non_trainable_variables,\n",
        "            inputs,\n",
        "            targets,\n",
        "            training=True,\n",
        "        )\n",
        "\n",
        "        (\n",
        "            trainable_variables,\n",
        "            optimizer_variables,\n",
        "        ) = self.optimizer.stateless_apply(\n",
        "            optimizer_variables, grads, trainable_variables\n",
        "        )\n",
        "\n",
        "        logs = {\"loss\": loss}\n",
        "        state = (\n",
        "            trainable_variables,\n",
        "            non_trainable_variables,\n",
        "            optimizer_variables,\n",
        "            metrics_variables,\n",
        "        )\n",
        "        return logs, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKOmVDTa5vKa"
      },
      "outputs": [],
      "source": [
        "%%backend jax\n",
        "def get_custom_model():\n",
        "    inputs = keras.Input(shape=(28 * 28,))\n",
        "    features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "    features = layers.Dropout(0.5)(features)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "    model = CustomModel(inputs, outputs)\n",
        "    model.compile(optimizer=keras.optimizers.Adam())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwIElYbV5vKa"
      },
      "outputs": [],
      "source": [
        "%%backend jax\n",
        "model = get_custom_model()\n",
        "model.fit(train_images, train_labels, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2NyAuVL5vKa"
      },
      "source": [
        "#### Handling metrics in a custom train_step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usMOhfu65vKa"
      },
      "source": [
        "##### train_step() metrics handling with TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2Mh6Wir5vKa"
      },
      "outputs": [],
      "source": [
        "%%backend tensorflow\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "    # 每个批次都会调用\n",
        "    def train_step(self, data):\n",
        "        inputs, targets = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = self.compute_loss(y=targets, y_pred=predictions)\n",
        "\n",
        "        # 向后传播，每个批次都会更新权重\n",
        "        gradients = tape.gradient(loss, self.trainable_weights)\n",
        "        self.optimizer.apply(gradients, self.trainable_weights)\n",
        "\n",
        "        for metric in self.metrics:\n",
        "            if metric.name == \"loss\":\n",
        "                metric.update_state(loss)\n",
        "            else:\n",
        "                metric.update_state(targets, predictions)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mfp9Yi415vKa"
      },
      "outputs": [],
      "source": [
        "%%backend tensorflow\n",
        "def get_custom_model():\n",
        "    inputs = keras.Input(shape=(28 * 28,))\n",
        "    features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "    features = layers.Dropout(0.5)(features)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "    model = CustomModel(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = get_custom_model()\n",
        "model.fit(train_images, train_labels, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 代码说明"
      ],
      "metadata": {
        "id": "FNpq_prR2hQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这段代码定义了一个自定义的 Keras 模型，重写了训练步骤以实现更灵活的训练过程。让我逐部分详细解释：\n",
        "\n",
        "####### 代码结构分析\n",
        "\n",
        "######## 1. 基础设置和导入\n",
        "```python\n",
        "%%backend tensorflow\n",
        "import keras\n",
        "from keras import layers\n",
        "```\n",
        "- `%%backend tensorflow`: 在 Jupyter notebook 中设置后端为 TensorFlow\n",
        "- 导入 Keras 框架和层模块\n",
        "\n",
        "######## 2. 自定义模型类定义\n",
        "```python\n",
        "class CustomModel(keras.Model):\n",
        "```\n",
        "创建一个继承自 `keras.Model` 的自定义模型类\n",
        "\n",
        "######## 3. 重写训练步骤方法\n",
        "```python\n",
        "def train_step(self, data):\n",
        "```\n",
        "重写 `train_step` 方法，这是训练过程中的核心方法，每个批次(batch)都会调用\n",
        "\n",
        "######## 4. 数据解包和前向传播\n",
        "```python\n",
        "inputs, targets = data\n",
        "with tf.GradientTape() as tape:\n",
        "    predictions = self(inputs, training=True)\n",
        "    loss = self.compute_loss(y=targets, y_pred=predictions)\n",
        "```\n",
        "- **数据解包**: `inputs, targets = data` - 将输入数据分为输入和标签\n",
        "- **梯度记录**: `tf.GradientTape()` - 创建梯度记录上下文，跟踪所有可训练变量的操作\n",
        "- **前向传播**: `self(inputs, training=True)` - 调用模型进行前向传播\n",
        "- **损失计算**: `self.compute_loss()` - 使用模型编译时定义的损失函数计算损失\n",
        "\n",
        "######## 5. 反向传播和权重更新\n",
        "```python\n",
        "gradients = tape.gradient(loss, self.trainable_weights)\n",
        "self.optimizer.apply(gradients, self.trainable_weights)\n",
        "```\n",
        "- **梯度计算**: `tape.gradient()` - 计算损失相对于所有可训练权重的梯度\n",
        "- **权重更新**: `optimizer.apply()` - 使用优化器应用梯度更新权重\n",
        "\n",
        "######## 6. 指标更新\n",
        "```python\n",
        "for metric in self.metrics:\n",
        "    if metric.name == \"loss\":\n",
        "        metric.update_state(loss)\n",
        "    else:\n",
        "        metric.update_state(targets, predictions)\n",
        "```\n",
        "- 遍历所有在模型编译时定义的指标\n",
        "- 如果是损失指标，直接使用计算出的损失值\n",
        "- 对于其他指标（如准确率），使用真实标签和预测值更新\n",
        "\n",
        "######## 7. 返回训练指标\n",
        "```python\n",
        "return {m.name: m.result() for m in self.metrics}\n",
        "```\n",
        "返回一个字典，包含所有指标的当前值\n",
        "\n",
        "####### 与标准训练的区别\n",
        "\n",
        "**标准 Keras 训练**：\n",
        "- 自动处理梯度计算和权重更新\n",
        "- 内置损失计算和指标更新\n",
        "\n",
        "**自定义训练的优势**：\n",
        "1. **灵活性**: 可以自定义训练逻辑\n",
        "2. **复杂场景**: 支持多任务学习、对抗训练等复杂场景\n",
        "3. **调试**: 更容易调试和监控训练过程\n",
        "4. **研究**: 适合研究和实验新的训练方法\n",
        "\n",
        "####### 使用示例\n",
        "```python\n",
        "# 创建自定义模型实例\n",
        "model = CustomModel()\n",
        "\n",
        "# 编译模型（定义损失函数、优化器和指标）\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy', 'loss']\n",
        ")\n",
        "\n",
        "# 正常训练\n",
        "model.fit(x_train, y_train, epochs=10)\n",
        "```\n",
        "\n",
        "这种自定义训练步骤的方式为复杂的训练场景提供了极大的灵活性。\n",
        "\n",
        "自定义训练时，需要自己计算指标吗？\n",
        "不用"
      ],
      "metadata": {
        "id": "7fNN_Y1Q2i54"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGrNvFA_5vKa"
      },
      "source": [
        "##### train_step() metrics handling with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcB3dr_R5vKa"
      },
      "outputs": [],
      "source": [
        "%%backend torch\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        inputs, targets = data\n",
        "        predictions = self(inputs, training=True)\n",
        "        loss = self.compute_loss(y=targets, y_pred=predictions)\n",
        "\n",
        "        loss.backward()\n",
        "        trainable_weights = [v for v in self.trainable_weights]\n",
        "        gradients = [v.value.grad for v in trainable_weights]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.optimizer.apply(gradients, trainable_weights)\n",
        "\n",
        "        for metric in self.metrics:\n",
        "            if metric.name == \"loss\":\n",
        "                metric.update_state(loss)\n",
        "            else:\n",
        "                metric.update_state(targets, predictions)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvzU7vxz5vKa"
      },
      "outputs": [],
      "source": [
        "%%backend torch\n",
        "def get_custom_model():\n",
        "    inputs = keras.Input(shape=(28 * 28,))\n",
        "    features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "    features = layers.Dropout(0.5)(features)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "    model = CustomModel(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = get_custom_model()\n",
        "model.fit(train_images, train_labels, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ray36Sd5vKa"
      },
      "source": [
        "##### train_step() metrics handling with JAX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi3g0rL65vKa"
      },
      "outputs": [],
      "source": [
        "%%backend jax\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "    def compute_loss_and_updates(\n",
        "        self,\n",
        "        trainable_variables,\n",
        "        non_trainable_variables,\n",
        "        inputs,\n",
        "        targets,\n",
        "        training=False,\n",
        "    ):\n",
        "        predictions, non_trainable_variables = self.stateless_call(\n",
        "            trainable_variables,\n",
        "            non_trainable_variables,\n",
        "            inputs,\n",
        "            training=training,\n",
        "        )\n",
        "        loss = self.compute_loss(y=targets, y_pred=predictions)\n",
        "        return loss, (predictions, non_trainable_variables)\n",
        "\n",
        "    def train_step(self, state, data):\n",
        "        (\n",
        "            trainable_variables,\n",
        "            non_trainable_variables,\n",
        "            optimizer_variables,\n",
        "            metrics_variables,\n",
        "        ) = state\n",
        "        inputs, targets = data\n",
        "\n",
        "        grad_fn = jax.value_and_grad(\n",
        "            self.compute_loss_and_updates, has_aux=True\n",
        "        )\n",
        "\n",
        "        (loss, (predictions, non_trainable_variables)), grads = grad_fn(\n",
        "            trainable_variables,\n",
        "            non_trainable_variables,\n",
        "            inputs,\n",
        "            targets,\n",
        "            training=True,\n",
        "        )\n",
        "        (\n",
        "            trainable_variables,\n",
        "            optimizer_variables,\n",
        "        ) = self.optimizer.stateless_apply(\n",
        "            optimizer_variables, grads, trainable_variables\n",
        "        )\n",
        "\n",
        "        new_metrics_vars = []\n",
        "        logs = {}\n",
        "        for metric in self.metrics:\n",
        "            num_prev = len(new_metrics_vars)\n",
        "            num_current = len(metric.variables)\n",
        "            current_vars = metrics_variables[num_prev : num_prev + num_current]\n",
        "            if metric.name == \"loss\":\n",
        "                current_vars = metric.stateless_update_state(current_vars, loss)\n",
        "            else:\n",
        "                current_vars = metric.stateless_update_state(\n",
        "                    current_vars, targets, predictions\n",
        "                )\n",
        "            logs[metric.name] = metric.stateless_result(current_vars)\n",
        "            new_metrics_vars += current_vars\n",
        "\n",
        "        state = (\n",
        "            trainable_variables,\n",
        "            non_trainable_variables,\n",
        "            optimizer_variables,\n",
        "            new_metrics_vars,\n",
        "        )\n",
        "        return logs, state"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "chapter07_deep-dive-keras",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}